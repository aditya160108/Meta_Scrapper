{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nj5Diit4yRKX"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def scrape_metadata(url):\n",
        "\n",
        "  try:\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    title = soup.find('title').text if soup.title else None\n",
        "    description = soup.find('meta', attrs={'name': 'description'})['content'] if soup.find('meta', attrs={'name': 'description'}) else None\n",
        "    keywords = soup.find('meta', attrs={'name': 'keywords'})['content'] if soup.find('meta', attrs={'name': 'keywords'}) else None\n",
        "\n",
        "    return title, description, keywords\n",
        "\n",
        "  except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error fetching URL: {e}\")\n",
        "    return None, None, None\n",
        "\n",
        "\n",
        "urls = input(\"Enter multiple URLs separated by commas: \").split(\",\")\n",
        "\n",
        "\n",
        "data = []\n",
        "\n",
        "for url in urls:\n",
        "  title, description, keywords = scrape_metadata(url.strip())\n",
        "  data.append({'URL': url, 'Title': title, 'Description': description, 'Keywords': keywords})\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "df.to_excel('scraped_metadata.xlsx', index=False)\n",
        "from google.colab import files\n",
        "files.download('scraped_metadata.xlsx')"
      ]
    }
  ]
}